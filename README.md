This repository contains my implementation of a Large Language Model (LLM) from scratch. It includes complete workflows for:

1) Tokenizer and dataset preparation

2) Model architecture (GPT-style transformer)

3) Pretraining from scratch on a raw text corpus

4) Post-training (instruction fine-tuning, coming soon).

5) This implementation is educational and optimized for clarity, not performance.

Based on the book “LLMs from Scratch” by @rasbt

Extendable to larger datasets, distributed training, and inference optimizations.
